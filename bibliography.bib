
@article{mehrabi2021survey,
  author  = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title   = {A Survey on Bias and Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  volume  = {54},
  number  = {6},
  pages   = {1--35},
  year    = {2021},
  doi     = {10.1145/3457607}
}


@article{corbett2023measure,
  author  = {Corbett-Davies, Sam and Goel, Sharad},
  title   = {The Measure and Mismeasure of Fairness},
  journal = {Journal of Machine Learning Research},
  volume  = {24},
  pages   = {1--117},
  year    = {2023},
  doi     = {10.48550/arXiv.1808.00023}
}


@InProceedings{buolamwini2018gender,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 Feb,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}



@misc{ensign2018runaway,
      title={Runaway Feedback Loops in Predictive Policing}, 
      author={Danielle Ensign and Sorelle A. Friedler and Scott Neville and Carlos Scheidegger and Suresh Venkatasubramanian},
      year={2017},
      eprint={1706.09847},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/1706.09847}, 
}


@article{caliskan2017semantics,
   title={Semantics derived automatically from language corpora contain human-like biases},
   volume={356},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.aal4230},
   DOI={10.1126/science.aal4230},
   number={6334},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
   year={2017},
   month=apr, pages={183–186} 
}


@inproceedings{suresh2021framework, 
   series={EAAMO ’21},
   title={A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
   url={http://dx.doi.org/10.1145/3465416.3483305},
   DOI={10.1145/3465416.3483305},
   booktitle={Equity and Access in Algorithms, Mechanisms, and Optimization},
   publisher={ACM},
   author={Suresh, Harini and Guttag, John},
   year={2021},
   month=oct, pages={1–9},
   collection={EAAMO ’21} 
}


@article{kamiran2013quantifying,
author = {Kamiran, Faisal and Zliobaitė, Indrė and Calders, Toon},
year = {2012},
month = {06},
pages = {in press},
title = {Quantifying explainable discrimination and removing illegal discrimination in automated decision making},
volume = {1},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-012-0584-8}
}


@inproceedings{verma2018fairness,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness definitions explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1–7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}


@article{mitchell2021algorithmic,
   author = "Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian",
   title = "Algorithmic Fairness: Choices, Assumptions, and Definitions", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2021",
   volume = "8",
   pages = "141-163",
   doi = "https://doi.org/10.1146/annurev-statistics-042720-125902",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-042720-125902",
   publisher = "Annual Reviews",
   issn = "2326-831X",
   type = "Journal Article",
   keywords = "algorithmic fairness",
   keywords = "machine learning",
   keywords = "decision theory",
   keywords = "predictive modeling",
   keywords = "statistical learning",
   abstract = "A recent wave of research has attempted to define fairness quantitatively. In particular, this work has explored what fairness might mean in the context of decisions based on the predictions of statistical and machine learning models. The rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions. This article attempts to bring much-needed order. First, we explicate the various choices and assumptions made—often implicitly—to justify the use of prediction-based decision-making. Next, we show how such choices and assumptions can raise fairness concerns and we present a notationally consistent catalog of fairness definitions from the literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision-making.",
  }


@misc{kusner2017counterfactual,
      title={Inherent Trade-Offs in the Fair Determination of Risk Scores}, 
      author={Jon Kleinberg and Sendhil Mullainathan and Manish Raghavan},
      year={2016},
      eprint={1609.05807},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.05807}, 
}


@article{chouldechova2020snapshot,
author = {Chouldechova, Alexandra and Roth, Aaron},
title = {A snapshot of the frontiers of fairness in machine learning},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3376898},
doi = {10.1145/3376898},
abstract = {A group of industry, academic, and government experts convene in Philadelphia to explore the roots of algorithmic bias.},
journal = {Commun. ACM},
month = apr,
pages = {82–89},
numpages = {8}
}


@article{bartlett2022consumer,
title = {Consumer-lending discrimination in the FinTech Era},
journal = {Journal of Financial Economics},
volume = {143},
number = {1},
pages = {30-56},
year = {2022},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2021.05.047},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002403},
author = {Robert Bartlett and Adair Morse and Richard Stanton and Nancy Wallace},
keywords = {Discrimination, FinTech, GSE mortgages, Credit scoring, Algorithmic underwriting, Big-data lending, Platform loans, Statistical discrimination, Legitimate business necessity},
abstract = {U.S. fair-lending law prohibits lenders from making credit determinations that disparately affect minority borrowers if those determinations are based on characteristics unrelated to creditworthiness. Using an identification under this rule, we show risk-equivalent Latinx/Black borrowers pay significantly higher interest rates on GSE-securitized and FHA-insured loans, particularly in high-minority-share neighborhoods. We estimate these rate differences cost minority borrowers over $450 million yearly. FinTech lenders’ rate disparities were similar to those of non-Fintech lenders for GSE mortgages, but lower for FHA mortgages issued in 2009–2015 and for FHA refi mortgages issued in 2018–2019.}
}


@misc{berk2017fairness,
      title={Fairness in Criminal Justice Risk Assessments: The State of the Art}, 
      author={Richard Berk and Hoda Heidari and Shahin Jabbari and Michael Kearns and Aaron Roth},
      year={2017},
      eprint={1703.09207},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.09207}, 
}


@inproceedings{deng2022exploring, 
   series={FAccT ’22},
   title={Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits},
   url={http://dx.doi.org/10.1145/3531146.3533113},
   DOI={10.1145/3531146.3533113},
   booktitle={2022 ACM Conference on Fairness Accountability and Transparency},
   publisher={ACM},
   author={Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
   year={2022},
   month=jun, 
   pages={473–484},
   collection={FAccT ’22} 
}


@article{obermeyer2019dissecting,
author = {Ziad Obermeyer  and Brian Powers  and Christine Vogeli  and Sendhil Mullainathan },
title = {Dissecting racial bias in an algorithm used to manage the health of populations},
journal = {Science},
volume = {366},
number = {6464},
pages = {447-453},
year = {2019},
doi = {10.1126/science.aax2342},
URL = {https://www.science.org/doi/abs/10.1126/science.aax2342},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aax2342},
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.}}


@misc{xian2024unified,
      title={A Unified Post-Processing Framework for Group Fairness in Classification}, 
      author={Ruicheng Xian and Han Zhao},
      year={2024},
      eprint={2405.04025},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.04025}, 
}


@misc{dwork2011fairnessawareness,
      title={Fairness Through Awareness}, 
      author={Cynthia Dwork and Moritz Hardt and Toniann Pitassi and Omer Reingold and Rich Zemel},
      year={2011},
      eprint={1104.3913},
      archivePrefix={arXiv},
      primaryClass={cs.CC},
      url={https://arxiv.org/abs/1104.3913}, 
}


@misc{feldman2015certifyingremovingdisparateimpact,
      title={Certifying and removing disparate impact}, 
      author={Michael Feldman and Sorelle Friedler and John Moeller and Carlos Scheidegger and Suresh Venkatasubramanian},
      year={2015},
      eprint={1412.3756},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1412.3756}, 
}


@misc{hardt2016equalityopportunitysupervisedlearning,
      title={Equality of Opportunity in Supervised Learning}, 
      author={Moritz Hardt and Eric Price and Nathan Srebro},
      year={2016},
      eprint={1610.02413},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1610.02413}, 
}


@InProceedings{pmlr-v28-zemel13,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 Jun,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}


@article{8907b030dc7644cabfab035645f9b9da,
title = "Data preprocessing techniques for classification without discrimination",
abstract = "Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data. Keywords: Classification – Preprocessing – Discrimination-aware data mining",
author = "F. Kamiran and T.G.K. Calders",
year = "2012",
doi = "10.1007/s10115-011-0463-8",
language = "English",
volume = "33",
pages = "1--33",
journal = "Knowledge and Information Systems",
issn = "0219-1377",
publisher = "Springer",
number = "1",
}


@inproceedings{fauci-aequitas2024,
    articleno = 8,
    author = {Magnini, Matteo and Ciatto, Giovanni and Calegari, Roberta and Omicini, Andrea},
    booktitle = {AEQUITAS 2024: Fairness and Bias in AI},
    dblp = {conf/aequitas/MagniniCCO24},
    editor = {Calegari, Roberta and Dignum, Virginia and O'Sullivan, Barry},
    iris = {11585/995740},
    keywords = {AI Fairness, FaUCI, in-processing, regularization, mitigation},
    month = oct,
    note = {Proceedings of the 2nd Workshop on Fairness and Bias in AI co-located with 27th European Conference on Artificial Intelligence (ECAI 2024)},
    numpages = 13,
    pages = {8:1--8:13},
    publisher = {CEUR-WS},
    scholar = {12743200180573703475},
    scopus = {2-s2.0-85210042328},
    series = {CEUR Workshop Proceedings},
    title = {Enforcing Fairness via Constraint Injection with {FaUCI}},
    url = {https://ceur-ws.org/Vol-3808/paper8.pdf},
    urlopenaccess = {https://ceur-ws.org/Vol-3808/paper8.pdf},
    urlpdf = {https://ceur-ws.org/Vol-3808/paper8.pdf},
    volume = 3808,
    year = 2024
}


@misc{zhang2018mitigatingunwantedbiasesadversarial,
      title={Mitigating Unwanted Biases with Adversarial Learning}, 
      author={Brian Hu Zhang and Blake Lemoine and Margaret Mitchell},
      year={2018},
      eprint={1801.07593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.07593}, 
}


@inproceedings{kamishima2012prejudiceremoverregularizer,
author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
year = {2012},
month = {09},
pages = {35-50},
title = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
isbn = {978-3-642-33485-6},
doi = {10.1007/978-3-642-33486-3_3}
}


@misc{celis2020classificationfairnessconstraintsmetaalgorithm,
      title={Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees}, 
      author={L. Elisa Celis and Lingxiao Huang and Vijay Keswani and Nisheeth K. Vishnoi},
      year={2020},
      eprint={1806.06055},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.06055}, 
}


@inproceedings{NIPS2017_b8b9c74a,
 author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Fairness and Calibration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
 volume = {30},
 year = {2017}
}


@INPROCEEDINGS{6413831,
  author={Kamiran, Faisal and Karim, Asim and Zhang, Xiangliang},
  booktitle={2012 IEEE 12th International Conference on Data Mining}, 
  title={Decision Theory for Discrimination-Aware Classification}, 
  year={2012},
  volume={},
  number={},
  pages={924-929},
  keywords={Accuracy;Probabilistic logic;Standards;Communities;Data mining;Decision trees;Logistics;social discrimination;classification;decision theory;ensembles},
  doi={10.1109/ICDM.2012.45}}


@misc{bellamy2019aif360,
      title={AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias}, 
      author={Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and Pranay Lohia and Jacquelyn Martino and Sameep Mehta and Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and John Richards and Diptikalyan Saha and Prasanna Sattigeri and Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
      year={2018},
      eprint={1810.01943},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1810.01943}, 
}


@techreport{bird2020fairlearn,
author = {Bird, Sarah and Dudík, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
title = {Fairlearn: A toolkit for assessing and improving fairness in AI},
institution = {Microsoft},
year = {2020},
month = May,
abstract = {We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.},
url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
number = {MSR-TR-2020-32},
}


@article{barocas2016,
  title={Big Data's Disparate Impact},
  author={Solon Barocas and Andrew D. Selbst},
  journal={California Law Review},
  year={2016},
  volume={104},
  pages={671},
  url={https://api.semanticscholar.org/CorpusID:143133374}
}


@inproceedings{corbettdavies2017,
author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
title = {Algorithmic Decision Making and the Cost of Fairness},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098095},
doi = {10.1145/3097983.3098095},
abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {797–806},
numpages = {10},
keywords = {algorithmic fairness, discrimination, disparate impact, pretrial detention, risk assessment},
location = {Halifax, NS, Canada},
series = {KDD '17}
}


@inproceedings{holstein2019, 
   series={CHI ’19},
   title={Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?},
   url={http://dx.doi.org/10.1145/3290605.3300830},
   DOI={10.1145/3290605.3300830},
   booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
   publisher={ACM},
   author={Holstein, Kenneth and Wortman Vaughan, Jennifer and Daumé, Hal and Dudik, Miro and Wallach, Hanna},
   year={2019},
   month=may, pages={1–16},
   collection={CHI ’19} }



@inproceedings{lee2021,
author = {Lee, Michelle Seng Ah and Singh, Jat},
title = {The Landscape and Gaps in Open Source Fairness Toolkits},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445261},
doi = {10.1145/3411764.3445261},
abstract = {With the surge in literature focusing on the assessment and mitigation of unfair outcomes in algorithms, several open source ‘fairness toolkits’ recently emerged to make such methods widely accessible. However, little studied are the differences in approach and capabilities of existing fairness toolkits, and their fit-for-purpose in commercial contexts. Towards this, this paper identifies the gaps between the existing open source fairness toolkit capabilities and the industry practitioners’ needs. Specifically, we undertake a comparative assessment of the strengths and weaknesses of six prominent open source fairness toolkits, and investigate the current landscape and gaps in fairness toolkits through an exploratory focus group, a semi-structured interview, and an anonymous survey of data science/machine learning (ML) practitioners. We identify several gaps between the toolkits’ capabilities and practitioner needs, highlighting areas requiring attention and future directions towards tooling that better support ‘fairness in practice.’},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {699},
numpages = {13},
keywords = {open source toolkits, fairness toolkits, fairness, bias mitigation, bias detection, bias, algorithmic fairness, algorithm auditing},
location = {Yokohama, Japan},
series = {CHI '21}
}


@article{veale2017,
author = {Michael Veale and Reuben Binns},
title ={Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data},
journal = {Big Data \& Society},
volume = {4},
number = {2},
pages = {2053951717743530},
year = {2017},
doi = {10.1177/2053951717743530},
URL = {  
        https://doi.org/10.1177/2053951717743530
},
eprint = {    
        https://doi.org/10.1177/2053951717743530
},
    abstract = { Decisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might not hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate emergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and capacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems. This paper presents and discusses three potential approaches to deal with such knowledge and information deficits in the context of fairer machine learning. Trusted third parties could selectively store data necessary for performing discrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner. Collaborative online platforms would allow diverse organisations to record, share and access contextual and experiential knowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpretable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world fairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and contextually grounded. Computational fairness tools are useful, but must be researched and developed in and with the messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real, near-term algorithmic harm. }
}


@article{Katare2025,
   title={Analyzing and Mitigating Bias for Vulnerable Road Users by Addressing Class Imbalance in Datasets},
   volume={6},
   ISSN={2687-7813},
   url={http://dx.doi.org/10.1109/OJITS.2025.3564558},
   DOI={10.1109/ojits.2025.3564558},
   journal={IEEE Open Journal of Intelligent Transportation Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Katare, Dewant and Noguero, David Solans and Park, Souneil and Kourtellis, Nicolas and Janssen, Marijn and Ding, Aaron Yi},
   year={2025},
   pages={590–604} 
}


@misc{Marathe2023,
      title={In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles}, 
      author={Aboli Marathe and Rahee Walambe and Ketan Kotecha},
      year={2023},
      eprint={2204.01062},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.01062}, 
}


@article{Yang2023,
author = {Yang, Jenny and Soltan, Andrew and Eyre, David and Yang, Yang and Clifton, David},
year = {2023},
month = {03},
pages = {55},
title = {An adversarial training framework for mitigating algorithmic biases in clinical machine learning},
volume = {6},
journal = {NPJ digital medicine},
doi = {10.1038/s41746-023-00805-y}
}


@misc{Zhu2023,
      title={Fairness-Sensitive Policy-Gradient Reinforcement Learning for Reducing Bias in Robotic Assistance}, 
      author={Jie Zhu and Mengsha Hu and Xueyao Liang and Amy Zhang and Ruoming Jin and Rui Liu},
      year={2023},
      eprint={2306.04167},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2306.04167}, 
}


@misc{Li2024,
      title={Bias Behind the Wheel: Fairness Testing of Autonomous Driving Systems}, 
      author={Xinyue Li and Zhenpeng Chen and Jie M. Zhang and Federica Sarro and Ying Zhang and Xuanzhe Liu},
      year={2024},
      eprint={2308.02935},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2308.02935}, 
}


@article{smuha2021,
author = {Mirsch Née Decker, Marie and Wegner, Laila and Leicht-Scholten, Carmen},
year = {2024},
month = {11},
title = {Procedural fairness in algorithmic decision-making: the role of public engagement},
volume = {27},
journal = {Ethics and Information Technology},
doi = {10.1007/s10676-024-09811-4}
}


@misc{kuppler2021distributivejusticefairnessmetrics,
      title={Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?}, 
      author={Matthias Kuppler and Christoph Kern and Ruben L. Bach and Frauke Kreuter},
      year={2021},
      eprint={2105.01441},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2105.01441}, 
}



@article{algorithmBono2021,
    author = {Bono, Teresa and Croxson, Karen and Giles, Adam},
    title = {Algorithmic fairness in credit scoring},
    journal = {Oxford Review of Economic Policy},
    volume = {37},
    number = {3},
    pages = {585-617},
    year = {2021},
    month = {09},
    abstract = {The use of machine learning as an input into decision-making is on the rise, owing to its ability to uncover hidden patterns in large data and improve prediction accuracy. Questions have been raised, however, about the potential distributional impacts of these technologies, with one concern being that they may perpetuate or even amplify human biases from the past. Exploiting detailed credit file data for 800,000 UK borrowers, we simulate a switch from a traditional (logit) credit scoring model to ensemble machine-learning methods. We confirm that machine-learning models are more accurate overall. We also find that they do as well as the simpler traditional model on relevant fairness criteria, where these criteria pertain to overall accuracy and error rates for population subgroups defined along protected or sensitive lines (gender, race, health status, and deprivation). We do observe some differences in the way credit-scoring models perform for different subgroups, but these manifest under a traditional modelling approach and switching to machine learning neither exacerbates nor eliminates these issues. The paper discusses some of the mechanical and data factors that may contribute to statistical fairness issues in the context of credit scoring.},
    issn = {0266-903X},
    doi = {10.1093/oxrep/grab020},
    url = {https://doi.org/10.1093/oxrep/grab020},
    eprint = {https://academic.oup.com/oxrep/article-pdf/37/3/585/40434197/grab020.pdf},
}
@article{Chen2023,
	abstract = {This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondents'experiences and perceptions of AI-driven recruitment applications and discrimination.},
	author = {Chen, Zhisheng},
	date = {2023-09-13},
	date-added = {2025-05-25 12:00:30 +0200},
	date-modified = {2025-05-25 12:00:30 +0200},
	doi = {10.1057/s41599-023-02079-x},
	id = {Chen2023},
	journal = {Humanities and Social Sciences Communications},
	number = {1},
	pages = {567},
	title = {Ethics and discrimination in artificial intelligence-enabled recruitment practices},
	url = {https://doi.org/10.1057/s41599-023-02079-x},
	volume = {10},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1057/s41599-023-02079-x}
}
@article{algorithmicdiscrimination2024,
	abstract = {The widespread usage of machine learning systems and econometric methods in the credit domain has transformed the decision-making process for evaluating loan applications. Automated analysis of credit applications diminishes the subjectivity of the decision-making process. On the other hand, since machine learning is based on past decisions recorded in the financial institutions'datasets, the process very often consolidates existing bias and prejudice against groups defined by race, sex, sexual orientation, and other attributes. Therefore, the interest in identifying, preventing, and mitigating algorithmic discrimination has grown exponentially in many areas, such as Computer Science, Economics, Law, and Social Science. We conducted a comprehensive systematic literature review to understand (1) the research settings, including the discrimination theory foundation, the legal framework, and the applicable fairness metric; (2) the addressed issues and solutions; and (3) the open challenges for potential future research. We explored five sources: ACM Digital Library, Google Scholar, IEEE Digital Library, Springer Link, and Scopus. Following inclusion and exclusion criteria, we selected 78 papers written in English and published between 2017 and 2022. According to the meta-analysis of this literature survey, algorithmic discrimination has been addressed mainly by looking at the CS, Law, and Economics perspectives. There has been great interest in this topic in the financial area, especially the discrimination in providing access to the mortgage market and differential treatment (different fees, number of parcels, and interest rates). Most attention has been devoted to the potential discrimination due to bias in the dataset. Researchers are still only dealing with direct discrimination, addressed by algorithmic fairness, while indirect discrimination (structural discrimination) has not received the same attention.},
	author = {Garcia, Ana Cristina Bicharra and Garcia, Marcio Gomes Pinto and Rigobon, Roberto},
	date = {2024-08-01},
	date-added = {2025-05-25 12:02:01 +0200},
	date-modified = {2025-05-25 12:02:01 +0200},
	doi = {10.1007/s00146-023-01676-3},
	id = {Garcia2024},
	journal = {AI \& SOCIETY},
	number = {4},
	pages = {2059--2098},
	title = {Algorithmic discrimination in the credit domain: what do we know about it?},
	url = {https://doi.org/10.1007/s00146-023-01676-3},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s00146-023-01676-3}
}
@misc{yuan2024ensuringfairnesstransparentauditing,
      title={Ensuring Fairness with Transparent Auditing of Quantitative Bias in AI Systems}, 
      author={Chih-Cheng Rex Yuan and Bow-Yaw Wang},
      year={2024},
      eprint={2409.06708},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2409.06708}, 
}
@misc{kheya2024pursuitfairnessartificialintelligence,
      title={The Pursuit of Fairness in Artificial Intelligence Models: A Survey}, 
      author={Tahsin Alamgir Kheya and Mohamed Reda Bouadjenek and Sunil Aryal},
      year={2024},
      eprint={2403.17333},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.17333}, 
}
@INPROCEEDINGS{10749873,
  author={Nathim, Khalida Walid and Hameed, Nada Abdulkareem and Salih, Saja Abdulfattah and Taher, Nada Adnan and Salman, Hayder Mahmood and Chornomordenko, Dmytro},
  booktitle={2024 36th Conference of Open Innovations Association (FRUCT)}, 
  title={Ethical AI with Balancing Bias Mitigation and Fairness in Machine Learning Models}, 
  year={2024},
  pages={797-807},
  keywords={Ethics;Adaptation models;Machine learning algorithms;Costs;Prevention and mitigation;Finance;Collaboration;Machine learning;Medical services;Context modeling},
  doi={10.23919/FRUCT64283.2024.10749873}
}
@article{Ferrara2023,
   title={Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies},
   volume={6},
   url={http://dx.doi.org/10.3390/sci6010003},
   DOI={10.3390/sci6010003},
   number={1},
   journal={Sci},
   publisher={MDPI AG},
   author={Ferrara, Emilio},
   year={2023},
   month=dec, pages={3} 
}

@INPROCEEDINGS{8880288,
  author={Sulaimon, Ismail Abiodun and Ghoneim, Ahmed and Alrashoud, Mubarak},
  booktitle={2019 8th International Conference on Modeling Simulation and Applied Optimization (ICMSAO)}, 
  title={A New Reinforcement Learning-Based Framework for Unbiased Autonomous Software Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Software algorithms;Machine learning algorithms;Prediction algorithms;Software systems;Artificial intelligence;Accidents;Autonomous automobiles;Software Engineering;Algorithmic-Bias (Algorithmic discrimination;Algorithmic Fairness);Artificial Intelligence (AI);Autonomous System;Machine Learning (ML);Reinforcement Learning},
  doi={10.1109/ICMSAO.2019.8880288}}

@inproceedings{ijcai2023p735,
  title     = {Assessing and Enforcing Fairness in the AI Lifecycle},
  author    = {Calegari, Roberta and G. Castañé, Gabriel and Milano, Michela and O'Sullivan, Barry},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {6554--6562},
  year      = {2023},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2023/735},
  url       = {https://doi.org/10.24963/ijcai.2023/735},
}

@online{carattino2019monkey,
  author  = {Aquiles Carattino},
  title   = {Monkey Patching and Its Consequences},
  year    = {2019},
  url     = {https://pythonforthelab.com/blog/monkey-patching-and-its-consequences},
  note    = {Accessed 2025-06-19}
}


@misc{beutel2017datadecisionstheoreticalimplications,
      title={Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations}, 
      author={Alex Beutel and Jilin Chen and Zhe Zhao and Ed H. Chi},
      year={2017},
      eprint={1707.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.00075}, 
}